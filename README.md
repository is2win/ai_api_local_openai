# Локальный API генерации ответов (GGUF model)

Этот проект представляет собой локальный API сервер для генерации ответов с использованием модели формата GGUF. Сервер реализован с помощью FastAPI и позволяет обрабатывать запросы, похожие на запросы OpenAI Chat API.

## Описание

Проект предоставляет один основной эндпоинт:

- **POST /v1/chat/completions**: Принимает JSON с запросом, содержащий модель, сообщения, температуру и максимальное количество токенов, обрабатывает запрос через локальную модель и возвращает сгенерированный ответ в формате, аналогичном формату OpenAI API.

В проекте используются следующие технологии:

- **FastAPI** для построения веб-сервера.
- **Pydantic** для валидации входных данных и формирования ответа.
- Модуль **GGUFModel** для загрузки и генерации текста с использованием модели.
- Библиотека **ctransformers** для генерации ответа на GPU.

## Структура проекта

- **main.py** – основной файл приложения, содержащий описание API и логику работы сервера.
- **model.py** – модуль для работы с моделью GGUF, ответственный за загрузку модели и генерацию текста.
- **venv/** – виртуальное окружение (при наличии).
- **.git/** – Git-репозиторий проекта.
- **applogs/** – директория для логов работы приложения и модуля модели.

## Установка

1. Создайте виртуальное окружение (если ещё не создано):

```bash
python3 -m venv venv
source venv/bin/activate
```

2. Установите зависимости. Необходимые пакеты:

- fastapi
- uvicorn
- pydantic
- ctransformers

```bash
pip install fastapi uvicorn pydantic ctransformers
```

## Настройка модели

В файле `main.py` измените переменную `model_path` на путь к вашей модели в формате GGUF:

```python
model_path = "path/to/your_model.gguf"
```

Убедитесь, что файл модели существует по указанному пути и что установлена библиотека `ctransformers`.

## Запуск сервера

Запустите сервер с помощью uvicorn:

```bash
uvicorn main:app --reload
```

Сервер будет доступен по адресу `http://127.0.0.1:8000`.

## Использование API

**Эндпоинт:** POST `/v1/chat/completions`

**Пример запроса:**

```json
{
  "model": "your-model-name",
  "messages": [
    {"role": "user", "content": "Привет, можешь помочь?"}
  ],
  "temperature": 0.7,
  "max_tokens": 150
}
```

**Пример ответа:**

```json
{
  "id": "chatcmpl-123456",
  "object": "chat.completion",
  "created": 1611234567,
  "choices": [
    {
      "message": {"role": "assistant", "content": "Привет! Чем я могу помочь?"},
      "index": 0,
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 10,
    "completion_tokens": 5,
    "total_tokens": 15
  }
}
```

## Логирование

Логи создаются в директории `applogs`:

- `main.log` – логи основного приложения.
- `model.log` – логи модуля модели.

## Примечания

- Перед запуском убедитесь, что все зависимости установлены и файл модели доступен по указанному пути.
- В случае ошибки генерации возвращается статус 500 с описанием ошибки.
- Для генерации текста используется библиотека `ctransformers`, для работы которой требуется GPU.

## Лицензия

Укажите здесь информацию о лицензии, если это необходимо. 